{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae47f8f-ed3f-44e9-b94d-fa58b02a1569",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca83a69a-8c31-4c2a-8beb-dae5938cc023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 22:30:22,195 Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-06-10 22:30:22,196 NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict as dd\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers.optimization import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, average_precision_score\n",
    "import logging\n",
    "import utils\n",
    "import settings\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import gc\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from awp import AWP\n",
    "from PST_model import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a1e4c7-38a7-4a4f-9a0d-60bef46baa81",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d20d54-6146-48d8-9543-61616b071f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建一个字典来存储配置参数\n",
    "config_dict = {\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
    "    \"NUM_TRAIN_EPOCHS\": 20,\n",
    "    \"LEARNING_RATE\": 2e-5,\n",
    "    \"WARMUP_PROPORTION\": 0.1,\n",
    "    \"MAX_GRAD_NORM\": 1000,\n",
    "    'differential_learning_rate': 5e-4,\n",
    "    'differential_learning_rate_layers': 'head',\n",
    "    # \"eps\": 1e-6,\n",
    "    # \"betas\": (0.9, 0.999),\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"seed\": 42,\n",
    "    \"MAX_SEQ_LENGTH\": 512,\n",
    "    'model_name': 'scibert',  # roberta-large, deberta-base, distilbert\n",
    "    'clean_flag': True,\n",
    "    'train_mask': False,\n",
    "    'dist': 200,\n",
    "    'optim_strategy': 'normal',  # normal, llrd\n",
    "    'PATIENCE': 3,\n",
    "    'Debug': False,\n",
    "    'scheduler': 'cosine',  # linear, cosine\n",
    "    'amp': True,\n",
    "    'pool': 'ConcatPool', # WLP, GeM, Mean, ConcatPool, AP\n",
    "    'loss_function': 'CrossEntropy',\n",
    "    # 'model_class': 'PST_model',\n",
    "    'BATCH_SIZE': 8,\n",
    "    'awp_enable': True,\n",
    "    'awp_start_epoch': 2,\n",
    "    'layer_start': 9,\n",
    "    'reinit_n_layers': 0,\n",
    "}\n",
    "\n",
    "# 将字典转换为 SimpleNamespace 对象\n",
    "cfg = SimpleNamespace(**config_dict)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg.device = device\n",
    "\n",
    "\n",
    "\n",
    "# 日志\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05985df0-4c58-49c8-aebe-d8c5800c1acc",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466b7dab-775c-4193-9033-8328dc79b293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dad9dc-4fcf-472c-a72a-f4879eee2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_input(cfg):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_valid = []\n",
    "    y_valid = []\n",
    "\n",
    "    data_dir = join(settings.DATA_TRACE_DIR, \"PST\")  # 文件路径\n",
    "    papers = utils.load_json(data_dir, \"paper_source_trace_train_ans.json\")\n",
    "    n_papers = len(papers)  # 训练集 788样本\n",
    "\n",
    "    papers = sorted(papers, key=lambda x: x[\"_id\"])  # 按id字段排序\n",
    "    n_train = int(n_papers * 2 / 3)\n",
    "    # n_valid = n_papers - n_train\n",
    "    # 划分，525训练，263验证\n",
    "    papers_train = papers[:n_train]\n",
    "    papers_valid = papers[n_train:]\n",
    "\n",
    "    # 获取每个paper的id\n",
    "    pids_train = {p[\"_id\"] for p in papers_train}\n",
    "    pids_valid = {p[\"_id\"] for p in papers_valid}\n",
    "\n",
    "    in_dir = join(data_dir, \"paper-xml\")  # .xml原文文件夹路径\n",
    "    files = []\n",
    "    for f in os.listdir(in_dir):\n",
    "        if f.endswith(\".xml\"):\n",
    "            files.append(f)  # 获取所有原文\n",
    "    # 把所有样本的源论文题目找出来\n",
    "    pid_to_source_titles = dd(list)\n",
    "    for paper in tqdm(papers):\n",
    "        pid = paper[\"_id\"]\n",
    "        for ref in paper[\"refs_trace\"]:\n",
    "            pid_to_source_titles[pid].append(ref[\"title\"].lower())\n",
    "\n",
    "    for cur_pid in tqdm(pids_train | pids_valid):\n",
    "        f = open(join(in_dir, cur_pid + \".xml\"), encoding='utf-8')\n",
    "        xml = f.read()\n",
    "        bs = BeautifulSoup(xml, \"xml\")  # 用BS解析xml内容\n",
    "\n",
    "        source_titles = pid_to_source_titles[cur_pid]\n",
    "        if len(source_titles) == 0:\n",
    "            continue\n",
    "\n",
    "        references = bs.find_all(\"biblStruct\")  # 找到所有完整的参考文献引用\n",
    "        bid_to_title = {}\n",
    "        n_refs = 0\n",
    "        for ref in references:\n",
    "            if \"xml:id\" not in ref.attrs:  # 没有id属性则跳过，所以实际数量可能会少于len(references)\n",
    "                continue\n",
    "            bid = ref.attrs[\"xml:id\"]  # bib ID\n",
    "            if ref.analytic is None:\n",
    "                continue\n",
    "            if ref.analytic.title is None:\n",
    "                continue\n",
    "            bid_to_title[bid] = ref.analytic.title.text.lower()  # 把ID和标题加入到字典中\n",
    "            b_idx = int(bid[1:]) + 1\n",
    "            if b_idx > n_refs:\n",
    "                n_refs = b_idx  # 更新参考文献数量\n",
    "        \n",
    "        flag = False\n",
    "\n",
    "        cur_pos_bib = set()\n",
    "\n",
    "        for bid in bid_to_title:\n",
    "            cur_ref_title = bid_to_title[bid]  # 获取当前参考文献题目\n",
    "            for label_title in source_titles:\n",
    "                if fuzz.ratio(cur_ref_title, label_title) >= 80:\n",
    "                    flag = True\n",
    "                    cur_pos_bib.add(bid)  # 使用fuzz.ratio()判断当前参考文献题目和refs_trace题目的相似度，大于80则加入到cur_pos_bib\n",
    "\n",
    "        # 获取不匹配的参考文献集合，negative\n",
    "        cur_neg_bib = set(bid_to_title.keys()) - cur_pos_bib\n",
    "        # 找不到匹配的文献则跳过当前循环\n",
    "        if not flag:\n",
    "            continue\n",
    "    \n",
    "        if len(cur_pos_bib) == 0 or len(cur_neg_bib) == 0:\n",
    "            continue\n",
    "    \n",
    "        bib_to_contexts = utils.find_bib_context(xml, clean_flag=cfg.clean_flag, dist=cfg.dist)  # 获取上下文信息\n",
    "\n",
    "        n_pos = len(cur_pos_bib)\n",
    "        n_neg = n_pos * 10\n",
    "        # 正负样本比例为1：10\n",
    "        cur_neg_bib_sample = np.random.choice(list(cur_neg_bib), n_neg, replace=True)\n",
    "\n",
    "        if cur_pid in pids_train:\n",
    "            cur_x = x_train\n",
    "            cur_y = y_train\n",
    "        elif cur_pid in pids_valid:\n",
    "            cur_x = x_valid\n",
    "            cur_y = y_valid\n",
    "        else:\n",
    "            continue\n",
    "            # raise Exception(\"cur_pid not in train/valid/test\")\n",
    "        \n",
    "        for bib in cur_pos_bib:\n",
    "            cur_context = \" \".join(bib_to_contexts[bib])\n",
    "            cur_x.append(cur_context)\n",
    "            cur_y.append(1)\n",
    "    \n",
    "        for bib in cur_neg_bib_sample:\n",
    "            cur_context = \" \".join(bib_to_contexts[bib])\n",
    "            cur_x.append(cur_context)\n",
    "            cur_y.append(0)\n",
    "            \n",
    "    print(\"len(x_train)\", len(x_train), \"len(x_valid)\", len(x_valid))\n",
    "    with open(join(data_dir, \"bib_context_train.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in x_train:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    with open(join(data_dir, \"bib_context_valid.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in x_valid:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    with open(join(data_dir, \"bib_context_train_label.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in y_train:\n",
    "            f.write(str(line) + \"\\n\")\n",
    "    \n",
    "    with open(join(data_dir, \"bib_context_valid_label.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in y_valid:\n",
    "            f.write(str(line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a2a109-2246-4340-91cb-c60970959ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "def random_masking(text, tokenizer, max_length=512, mask_ratio=0.15):\n",
    "    # 对文本进行分词\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # 确保分词后的序列长度小于最大序列长度\n",
    "    tokens = tokens[:(max_length - 2)]  # 减2是因为要加上[CLS]和[SEP]标记\n",
    "\n",
    "    # 计算需要掩码的Token数量\n",
    "    num_tokens = len(tokens)\n",
    "    num_to_mask = int(mask_ratio * num_tokens)\n",
    "\n",
    "    # 随机选择Token进行掩码\n",
    "    indices_to_mask = np.random.choice(range(num_tokens), num_to_mask, replace=False)\n",
    "\n",
    "    # 将选中的Token替换为[MASK]\n",
    "    for i in sorted(indices_to_mask, reverse=True):\n",
    "        tokens[i] = tokenizer.mask_token\n",
    "\n",
    "    # 添加BERT的序列标记\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "    # 将Token转换为ID\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, max_seq_length, tokenizer, mask, verbose=0):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    # print(f'Mask Flag:{mask}')\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        # 分词和token化\n",
    "        # 随机掩码\n",
    "        if mask:\n",
    "            ratio = 0.15\n",
    "            input_ids = random_masking(text, tokenizer, max_seq_length, mask_ratio=ratio)\n",
    "            # print(f'Input have been masked with ratio {ratio}')\n",
    "        else:\n",
    "            input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "            if len(input_ids) > max_seq_length:\n",
    "                input_ids = input_ids[:max_seq_length]\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)  # 只有一个序列\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        # 确保序列长度一致\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "        \n",
    "    return input_items\n",
    "\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a471cf1e-6a03-4b08-b964-98491295f82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_texts = []\n",
    "    dev_texts = []\n",
    "    train_labels = []\n",
    "    dev_labels = []\n",
    "    data_year_dir = join(settings.DATA_TRACE_DIR, \"PST\")\n",
    "    print(\"data_year_dir\", data_year_dir)\n",
    "    with open(join(data_year_dir, \"bib_context_train.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            train_texts.append(line.strip())\n",
    "    with open(join(data_year_dir, \"bib_context_valid.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            dev_texts.append(line.strip())\n",
    "    with open(join(data_year_dir, \"bib_context_train_label.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            train_labels.append(int(line.strip()))\n",
    "    with open(join(data_year_dir, \"bib_context_valid_label.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            dev_labels.append(int(line.strip()))\n",
    "    \n",
    "    return train_texts, dev_texts, train_labels, dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0687fca0-7ea9-445f-a249-e36869d691fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scheduler(cfg, optimizer):\n",
    "    # 选择scheduler\n",
    "    if cfg.scheduler == \"linear\":\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=cfg.num_warmup_steps,\n",
    "            num_training_steps=cfg.num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == \"cosine\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=cfg.num_warmup_steps,\n",
    "            num_training_steps=cfg.num_train_steps\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75674cd-aa97-490d-9b61-3ade2b01f83a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_grouped_parameters(cfg, model):\n",
    "\n",
    "    model_type = \"model\"\n",
    "    learning_rate = 1.0e-5\n",
    "    weight_decay = 0.01\n",
    "    layerwise_learning_rate_decay = 0.9 # 0.8 train2\n",
    "    \n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    # initialize lr for task specific layer\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"head\" in n or \"pooling\" in n],\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"lr\": learning_rate,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # initialize lrs for every layer\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer)\n",
    "    layers.reverse()\n",
    "    \n",
    "    lr = learning_rate\n",
    "    for layer in layers:\n",
    "        lr *= layerwise_learning_rate_decay\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"lr\": lr,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"lr\": lr,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "    adam_epsilon = 1e-6 # 5e-6 train.sh\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        eps=adam_epsilon,\n",
    "    )\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfab8f60-5a75-41c6-9d98-5d28d19b03ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer(cfg, model): # best\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    differential_layers = cfg.differential_learning_rate_layers\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    param\n",
    "                    for name, param in model.named_parameters()\n",
    "                    if (not any(layer in name for layer in differential_layers))\n",
    "                    and (not any(nd in name for nd in no_decay))\n",
    "                ],\n",
    "                \"lr\": cfg.LEARNING_RATE,\n",
    "                \"weight_decay\": cfg.weight_decay,\n",
    "                # \"weight_decay\": 0.1,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    param\n",
    "                    for name, param in model.named_parameters()\n",
    "                    if (not any(layer in name for layer in differential_layers))\n",
    "                    and (any(nd in name for nd in no_decay))\n",
    "                ],\n",
    "                \"lr\": cfg.LEARNING_RATE,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    param\n",
    "                    for name, param in model.named_parameters()\n",
    "                    if (any(layer in name for layer in differential_layers))\n",
    "                    and (not any(nd in name for nd in no_decay))\n",
    "                ],\n",
    "                \"lr\": cfg.differential_learning_rate,\n",
    "                \"weight_decay\": cfg.weight_decay,\n",
    "                # \"weight_decay\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    param\n",
    "                    for name, param in model.named_parameters()\n",
    "                    if (any(layer in name for layer in differential_layers))\n",
    "                    and (any(nd in name for nd in no_decay))\n",
    "                ],\n",
    "                \"lr\": cfg.differential_learning_rate,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "        ],\n",
    "        lr=cfg.LEARNING_RATE,\n",
    "        # weight_decay=cfg.optimizer.weight_decay,\n",
    "        eps=1e-6,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6960c31-99b8-4051-b972-b6d0f8a6d31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 打印模型的参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99512289-a6a2-448b-b0b3-0e9ebfb06a46",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba0ce3f-ed78-4994-9ae1-bd607e360d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, criterion):\n",
    "    losses = utils.AverageMeter()\n",
    "    model.eval()\n",
    "    # eval_loss = 0\n",
    "    # nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "    # output_prob = []\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\", position=0)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        inputs = {}\n",
    "        inputs['input_ids'] = batch[0]\n",
    "        inputs['attention_mask'] = batch[1]\n",
    "        inputs['segment_ids'] = batch[2]\n",
    "        inputs['target'] = batch[3]\n",
    "        num_samples = inputs['target'].shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_eval = model(inputs)\n",
    "            loss = output_eval['loss']\n",
    "            logits = output_eval['logits']\n",
    "        losses.update(loss.item(), num_samples)\n",
    "        if step % 100 == 0:\n",
    "            tqdm.write(f\"Step {step}\")\n",
    "        outputs = np.argmax(logits.to('cpu'), axis=1)\n",
    "        label_ids = inputs['target'].to('cpu').numpy()\n",
    "        # prob = [float(utils.sigmoid(x)) for x in logits[:, 1].to('cpu').detach().numpy()]\n",
    "        # output_prob += prob\n",
    "        predicted_labels += list(outputs)\n",
    "        correct_labels += list(label_ids)\n",
    "    del inputs\n",
    "    clean_memory()\n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    # output_prob = np.array(output_prob)\n",
    "\n",
    "    return losses.avg, correct_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3623ea4-9796-4c60-a728-86ad6b79559b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36de7133-dac9-412f-a012-6e9d184eb070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(cfg, fold):\n",
    "    print(f\"model_name: {cfg.model_name}\")  # 模型名称\n",
    "\n",
    "    # ====================================================\n",
    "    # 加载数据\n",
    "    # ====================================================\n",
    "    train_texts, dev_texts, train_labels, dev_labels = load_data()\n",
    "    class_weight = len(train_labels) / (2 * np.bincount(train_labels))\n",
    "    class_weight = torch.Tensor(class_weight).to(device)\n",
    "    print(\"Class weight:\", class_weight)  # 计算每个类别的频次，平衡数据集中的类别\n",
    "    cfg.class_weight = class_weight\n",
    "\n",
    "    # ====================================================\n",
    "    # Debug\n",
    "    # ====================================================\n",
    "    if cfg.Debug:\n",
    "        train_texts, dev_texts, train_labels, dev_labels = train_texts[:100], dev_texts[:100], train_labels[:100], dev_labels[:100]\n",
    "        # cfg.NUM_TRAIN_EPOCHS = 1\n",
    "    print(\"Train size:\", len(train_texts))  # 7645\n",
    "    print(\"Dev size:\", len(dev_texts))  # 4037\n",
    "\n",
    "    # ====================================================\n",
    "    # 加载模型\n",
    "    # ====================================================\n",
    "    if cfg.model_name == \"deberta-base\":\n",
    "        model_path = './bert_models/deberta_v3_base'\n",
    "        \n",
    "    elif cfg.model_name == \"scibert\":\n",
    "        model_path = './bert_models/scibert_scivocab_uncased'\n",
    "        \n",
    "    elif cfg.model_name == 'roberta-base': \n",
    "\n",
    "        model_path = './bert_models/dsp_roberta_base_dapt_cs_tapt_sciie_3219'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = Net(cfg, model_path=model_path)\n",
    "    model.to(cfg.device)\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"模型的总参数量: {num_params}\")\n",
    "    # ====================================================\n",
    "    # Data_loader\n",
    "    # ====================================================\n",
    "    train_features = convert_examples_to_inputs(train_texts, train_labels, cfg.MAX_SEQ_LENGTH, tokenizer, mask=cfg.train_mask)\n",
    "    dev_features = convert_examples_to_inputs(dev_texts, dev_labels, cfg.MAX_SEQ_LENGTH, tokenizer, mask=False)\n",
    "    train_dataloader = get_data_loader(train_features, cfg.MAX_SEQ_LENGTH, cfg.BATCH_SIZE, shuffle=True)\n",
    "    dev_dataloader = get_data_loader(dev_features, cfg.MAX_SEQ_LENGTH, cfg.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # 优化器设置\n",
    "    # ====================================================\n",
    "    if cfg.optim_strategy == 'llrd':\n",
    "        print(\"Learning Rate Decay\")\n",
    "        optimizer = get_optimizer_grouped_parameters(cfg, model)\n",
    "    else:\n",
    "        optimizer = get_optimizer(cfg, model)\n",
    "    \n",
    "    num_train_steps = int(len(train_dataloader.dataset) / cfg.BATCH_SIZE / cfg.GRADIENT_ACCUMULATION_STEPS * cfg.NUM_TRAIN_EPOCHS)\n",
    "    num_warmup_steps = int(cfg.WARMUP_PROPORTION * num_train_steps)\n",
    "    cfg.num_train_steps = num_train_steps\n",
    "    cfg.num_warmup_steps = num_warmup_steps\n",
    "    scheduler = get_scheduler(cfg, optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # 其他设置\n",
    "    # ====================================================\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    if cfg.loss_function == 'CrossEntropy':\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=cfg.class_weight)\n",
    "    else:\n",
    "        criterion = FocalLoss(weight=cfg.class_weight)\n",
    "        \n",
    "    OUTPUT_DIR = join(settings.OUT_DIR, \"kddcup\", cfg.model_name, 'num_fold=0')\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "    loss_history = []\n",
    "    no_improvement = 0\n",
    "\n",
    "    awp = AWP(model,\n",
    "              optimizer,\n",
    "              adv_lr=0.0001,\n",
    "              adv_eps=0.001,\n",
    "              start_epoch=2,\n",
    "              scaler=scaler\n",
    "             )\n",
    "\n",
    "    for e in trange(int(cfg.NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "        model.train()\n",
    "\n",
    "        losses = utils.AverageMeter()\n",
    "        # ====================================================\n",
    "        # Trian\n",
    "        # ====================================================\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\", position=0)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {}\n",
    "            inputs['input_ids'] = batch[0]\n",
    "            inputs['attention_mask'] = batch[1]\n",
    "            inputs['segment_ids'] = batch[2]\n",
    "            inputs['target'] = batch[3]\n",
    "\n",
    "            num_samples = inputs['target'].shape[0]\n",
    "\n",
    "            with autocast():\n",
    "                output_dict = model(inputs)\n",
    "                loss = output_dict[\"loss\"]\n",
    "            if cfg.awp_enable and e >= cfg.awp_start_epoch:\n",
    "                awp.attack_backward(inputs)\n",
    "\n",
    "            if cfg.GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                loss = loss / cfg.GRADIENT_ACCUMULATION_STEPS\n",
    "            if step % 100 == 0:\n",
    "                tqdm.write(f\"Step {step}\")\n",
    "                \n",
    "            losses.update(loss.item(), num_samples)\n",
    "            scaler.scale(loss).backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.MAX_GRAD_NORM)\n",
    "\n",
    "            if (step + 1) % cfg.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "        # ====================================================\n",
    "        # Valid\n",
    "        # ====================================================\n",
    "        avg_loss = losses.avg\n",
    "        avg_dev_loss, true, prediction = evaluate(model, dev_dataloader, cfg.device, criterion)\n",
    "        print(\n",
    "              f'Train_Loss: {avg_loss:.4f}\\n'\n",
    "              f'Dev_Loss: {avg_dev_loss:.4f}\\n'\n",
    "              f'Grad: {grad_norm:.4f}\\n'\n",
    "              f'LR: {scheduler.get_lr()[0]:.8f}\\n'\n",
    "              f'Loss_History: {loss_history}'\n",
    "        )\n",
    "        # ====================================================\n",
    "        # 保存模型、早停\n",
    "        # ====================================================\n",
    "        if len(loss_history) == 0 or avg_dev_loss < min(loss_history):\n",
    "            no_improvement = 0\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "            print(\"Best model saved.\")\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= cfg.PATIENCE:\n",
    "            print(\"No improvement on development set. Finish training.\")\n",
    "            break\n",
    "        del inputs, losses, output_dict, batch\n",
    "        clean_memory()\n",
    "\n",
    "        loss_history.append(avg_dev_loss)\n",
    "\n",
    "    # 定义要保存的文件名\n",
    "    file_name = f'{OUTPUT_DIR}/Loss_history.txt'\n",
    "    # 打开文件用于写入\n",
    "    with open(file_name, \"w\") as file:\n",
    "        # 遍历列表中的每个元素\n",
    "        for loss in loss_history:\n",
    "            # 将数字转换为字符串，并写入文件，每个损失值占一行\n",
    "            file.write(str(loss) + \"\\n\")  # \\n 表示换行\n",
    "    print(f\"Loss history has been saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857b8a2-402c-4a18-8be6-a6016f4953b7",
   "metadata": {},
   "source": [
    "# Generate Valid Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1deebd4-230c-48e2-b95c-5fa8e4ef4e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_kddcup_valid_submission_bert(cfg):\n",
    "    print(\"model name\", cfg.model_name)\n",
    "    print(f'The length of context text:{cfg.dist}')\n",
    "    data_dir = join(settings.DATA_TRACE_DIR, \"PST\")\n",
    "    papers = utils.load_json(data_dir, \"paper_source_trace_valid_wo_ans.json\")  # 读取test paper id\n",
    "\n",
    "    if cfg.model_name == \"deberta-base\":\n",
    "        model_path = './bert_models/deberta_v3_base'\n",
    "\n",
    "    elif cfg.model_name == \"scibert\":\n",
    "        model_path = './bert_models/scibert_scivocab_uncased'\n",
    "\n",
    "    elif cfg.model_name == 'roberta-base':\n",
    "        model_path = './bert_models/dsp_roberta_base_dapt_cs_tapt_sciie_3219'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    sub_example_dict = utils.load_json(data_dir, \"submission_example_valid.json\")  # 提交模板template\n",
    "    print(\"device\", cfg.device)\n",
    "    cfg.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = Net(cfg, model_path=model_path)\n",
    "    OUTPUT_DIR = join(settings.OUT_DIR, \"kddcup\", cfg.model_name, 'num_fold=0')\n",
    "    model.load_state_dict(torch.load(join(OUTPUT_DIR, \"pytorch_model.bin\")))\n",
    "    model.to(cfg.device)\n",
    "    model.eval()\n",
    "    xml_dir = join(data_dir, \"paper-xml\")\n",
    "    sub_dict = {}\n",
    "\n",
    "    for paper in tqdm(papers):\n",
    "        cur_pid = paper[\"_id\"]\n",
    "        file = join(xml_dir, cur_pid + \".xml\")\n",
    "        f = open(file, encoding='utf-8')\n",
    "        xml = f.read()\n",
    "        bs = BeautifulSoup(xml, \"xml\")\n",
    "        f.close()\n",
    "\n",
    "        references = bs.find_all(\"biblStruct\")\n",
    "        bid_to_title = {}\n",
    "        n_refs = 0\n",
    "        for ref in references:\n",
    "            if \"xml:id\" not in ref.attrs:\n",
    "                continue\n",
    "            bid = ref.attrs[\"xml:id\"]\n",
    "            if ref.analytic is None:\n",
    "                continue\n",
    "            if ref.analytic.title is None:\n",
    "                continue\n",
    "            bid_to_title[bid] = ref.analytic.title.text.lower()  # 标题\n",
    "            b_idx = int(bid[1:]) + 1\n",
    "            if b_idx > n_refs:\n",
    "                n_refs = b_idx\n",
    "\n",
    "        bib_to_contexts = utils.find_bib_context(xml, clean_flag=cfg.clean_flag, dist=cfg.dist)\n",
    "        bib_sorted = [\"b\" + str(ii) for ii in range(n_refs)]  # 按顺序的bid\n",
    "        \n",
    "        y_score = [0] * n_refs\n",
    "\n",
    "        assert len(sub_example_dict[cur_pid]) == n_refs\n",
    "        # continue\n",
    "\n",
    "        contexts_sorted = [\" \".join(bib_to_contexts[bib]) for bib in bib_sorted]\n",
    "\n",
    "        test_features = convert_examples_to_inputs(contexts_sorted, y_score, cfg.MAX_SEQ_LENGTH, cfg.tokenizer, mask=False)\n",
    "        test_dataloader = get_data_loader(test_features, cfg.MAX_SEQ_LENGTH, 64, shuffle=False)\n",
    "\n",
    "        predicted_scores = []\n",
    "\n",
    "        with torch.inference_mode(mode=True):\n",
    "            for step, batch in enumerate(test_dataloader):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                # input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                inputs = {}\n",
    "                inputs['input_ids'] = batch[0]\n",
    "                inputs['attention_mask'] = batch[1]\n",
    "                inputs['segment_ids'] = batch[2]\n",
    "                inputs['target'] = batch[3]\n",
    "\n",
    "                with autocast():  # 放在循环内部\n",
    "                    output_dict = model(inputs)\n",
    "                    # tmp_eval_loss = r[0]\n",
    "                    logits = output_dict['logits']\n",
    "\n",
    "                cur_pred_scores = logits[:, 1].to('cpu').numpy()\n",
    "                predicted_scores.extend(cur_pred_scores)\n",
    "                del inputs\n",
    "                clean_memory()\n",
    "        for ii in range(len(predicted_scores)):\n",
    "            bib_idx = int(bib_sorted[ii][1:])\n",
    "            # print(\"bib_idx\", bib_idx)\n",
    "            y_score[bib_idx] = float(utils.sigmoid(predicted_scores[ii]))\n",
    "        \n",
    "        sub_dict[cur_pid] = y_score\n",
    "    \n",
    "    utils.dump_json(sub_dict, OUTPUT_DIR, f\"valid_submission_{cfg.model_name}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97fa0a-bfd3-419e-b7d3-4c408f426f41",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1831ec4b-dbbe-44a8-91fd-cfeed8843d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "fold = 0 \n",
    "utils.seed_everything(cfg.seed)\n",
    "\n",
    "train_loop(cfg, fold=0)\n",
    "\n",
    "# gen_kddcup_valid_submission_bert(cfg)\n",
    "\n",
    "# gen_kddcup_test_submission_bert(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9419e2e-4c54-430b-a25d-e83b8b970bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
