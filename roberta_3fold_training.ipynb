{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2544f8aa-eac1-44e5-b6a8-73c8133cfeaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "roberta 参数量：234567172"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2bfc0-3a97-4107-8287-c01c41fcb0df",
   "metadata": {},
   "source": [
    "# lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4805c9b-5b40-46ba-aa3a-47af7731d7e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 15:31:10,232 Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-06-12 15:31:10,233 NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict as dd\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, DebertaV2ForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers.optimization import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, average_precision_score\n",
    "import logging\n",
    "\n",
    "import utils\n",
    "import settings\n",
    "\n",
    "### add \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold \n",
    "import gc\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c2a9d-2171-49d0-923a-59fc8c94296c",
   "metadata": {},
   "source": [
    "# func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef79599-2c89-468d-bc92-3d92c11e5cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global is_clean, fuzz_ratio, model_name\n",
    "model_name = 'roberta-base'\n",
    "is_clean = False\n",
    "fuzz_ratio = 80\n",
    "NFOLDS = 4\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Function to clean RAM & vRAM\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    # ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "def adjust_lr(optimizer, epoch):\n",
    "    if epoch >= 2:\n",
    "        lr = 2e-5\n",
    "    else:\n",
    "        lr = 1e-6\n",
    "        \n",
    "    lr *= 4\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    optimizer.param_groups[1]['lr'] = 100*lr\n",
    "    \n",
    "    return lr\n",
    "\n",
    "def get_optimizer(net):\n",
    "    params = [x[1] for x in filter(lambda kv: \"bert\" in kv[0], net.named_parameters())]\n",
    "    arc_weight = [x[1] for x in filter(lambda kv: \"bert\" not in kv[0], net.named_parameters())]\n",
    "\n",
    "    optimizer = torch.optim.AdamW([{\"params\": params}, {\"params\": arc_weight}], lr=3e-4, betas=(0.9, 0.999),\n",
    "                                 eps=1e-08)\n",
    "    return optimizer\n",
    "\n",
    "def clean_text_line(line): \n",
    "    ## 得到 Abstract 信息\n",
    "    out_text = re.sub('<[^>]*>', '', line)\n",
    "    out_text = re.sub(r'\\t+', '\\t', out_text)\n",
    "    out_text = re.sub(r'\\n+', '\\n', out_text)\n",
    "    out_text = re.sub(r'[\\n\\t]+', '\\n', out_text)\n",
    "    return out_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f817c-6713-478a-81f5-131bda73916a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# bert input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7ac09-f5cd-4d38-a2c4-9d0aafbb8470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_bert_input():\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_valid = []\n",
    "    y_valid = []\n",
    "\n",
    "    data_dir = join(settings.DATA_TRACE_DIR, \"PST\")\n",
    "    papers = utils.load_json(data_dir, \"paper_source_trace_train_ans.json\")\n",
    "    n_papers = len(papers)\n",
    "    papers = sorted(papers, key=lambda x: x[\"_id\"])\n",
    "    n_train = int(n_papers * 2 / 3)\n",
    "    # n_valid = n_papers - n_train\n",
    "\n",
    "    papers_train = papers[:n_train]\n",
    "    papers_valid = papers[n_train:]\n",
    "\n",
    "    pids_train = {p[\"_id\"] for p in papers_train}\n",
    "    pids_valid = {p[\"_id\"] for p in papers_valid}\n",
    "\n",
    "    in_dir = join(data_dir, \"paper-xml\")\n",
    "    files = []\n",
    "    for f in os.listdir(in_dir):\n",
    "        if f.endswith(\".xml\"):\n",
    "            files.append(f)\n",
    "\n",
    "    pid_to_source_titles = dd(list)\n",
    "    for paper in tqdm(papers):\n",
    "        pid = paper[\"_id\"]\n",
    "        for ref in paper[\"refs_trace\"]:\n",
    "            pid_to_source_titles[pid].append(ref[\"title\"].lower())\n",
    "\n",
    "    # files = sorted(files)\n",
    "    # for file in tqdm(files):\n",
    "    for cur_pid in tqdm(pids_train | pids_valid):\n",
    "        # cur_pid = file.split(\".\")[0]\n",
    "        # if cur_pid not in pids_train and cur_pid not in pids_valid:\n",
    "            # continue\n",
    "        f = open(join(in_dir, cur_pid + \".xml\"), encoding='utf-8')\n",
    "        xml = f.read()\n",
    "        bs = BeautifulSoup(xml, \"xml\")\n",
    "\n",
    "        source_titles = pid_to_source_titles[cur_pid]\n",
    "        if len(source_titles) == 0:\n",
    "            continue\n",
    "\n",
    "        references = bs.find_all(\"biblStruct\")\n",
    "        bid_to_title = {}\n",
    "        n_refs = 0\n",
    "        for ref in references:\n",
    "            if \"xml:id\" not in ref.attrs:\n",
    "                continue\n",
    "            bid = ref.attrs[\"xml:id\"]\n",
    "            if ref.analytic is None:\n",
    "                continue\n",
    "            if ref.analytic.title is None:\n",
    "                continue\n",
    "            bid_to_title[bid] = ref.analytic.title.text.lower()\n",
    "            b_idx = int(bid[1:]) + 1\n",
    "            if b_idx > n_refs:\n",
    "                n_refs = b_idx\n",
    "        \n",
    "        flag = False\n",
    "\n",
    "        cur_pos_bib = set()\n",
    "\n",
    "        for bid in bid_to_title:\n",
    "            cur_ref_title = bid_to_title[bid]\n",
    "            for label_title in source_titles:\n",
    "                if fuzz.ratio(cur_ref_title, label_title) >= fuzz_ratio:\n",
    "                    flag = True\n",
    "                    cur_pos_bib.add(bid)\n",
    "        \n",
    "        cur_neg_bib = set(bid_to_title.keys()) - cur_pos_bib\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "    \n",
    "        if len(cur_pos_bib) == 0 or len(cur_neg_bib) == 0:\n",
    "            continue\n",
    "    \n",
    "        bib_to_contexts = utils.find_bib_context(xml)\n",
    "\n",
    "        n_pos = len(cur_pos_bib)\n",
    "        n_neg = n_pos * 10\n",
    "        cur_neg_bib_sample = np.random.choice(list(cur_neg_bib), n_neg, replace=True)\n",
    "\n",
    "        if cur_pid in pids_train:\n",
    "            cur_x = x_train\n",
    "            cur_y = y_train\n",
    "        elif cur_pid in pids_valid:\n",
    "            cur_x = x_valid\n",
    "            cur_y = y_valid\n",
    "        else:\n",
    "            continue\n",
    "            # raise Exception(\"cur_pid not in train/valid/test\")\n",
    "        \n",
    "        for bib in cur_pos_bib:\n",
    "            cur_context = \" \".join(bib_to_contexts[bib])\n",
    "            cur_x.append(cur_context)\n",
    "            cur_y.append(1)\n",
    "    \n",
    "        for bib in cur_neg_bib_sample:\n",
    "            cur_context = \" \".join(bib_to_contexts[bib])\n",
    "            cur_x.append(cur_context)\n",
    "            cur_y.append(0)\n",
    "    \n",
    "    print(\"len(x_train)\", len(x_train), \"len(x_valid)\", len(x_valid))\n",
    "\n",
    "\n",
    "    with open(join(data_dir, \"bib_context_train.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in x_train:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    with open(join(data_dir, \"bib_context_valid.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in x_valid:\n",
    "            f.write(line + \"\\n\")\n",
    "    \n",
    "    with open(join(data_dir, \"bib_context_train_label.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in y_train:\n",
    "            f.write(str(line) + \"\\n\")\n",
    "    \n",
    "    with open(join(data_dir, \"bib_context_valid_label.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in y_valid:\n",
    "            f.write(str(line) + \"\\n\")\n",
    "\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, max_seq_length, tokenizer, verbose=0):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "        \n",
    "    return input_items\n",
    "\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size, num_workers=8)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66341e7f-4bee-4d28-9961-3c10fcfdcec6",
   "metadata": {},
   "source": [
    "# 评估 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb23fe-1f7f-4904-a88e-702c3ea256d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            r = model(input_ids, \n",
    "                      attention_mask=input_mask,\n",
    "                      token_type_ids=segment_ids,\n",
    "                      labels=label_ids)\n",
    "            # tmp_eval_loss = r[0]\n",
    "            logits = r[1]\n",
    "            # print(\"logits\", logits)\n",
    "            tmp_eval_loss = criterion(logits, label_ids)\n",
    "\n",
    "        outputs = np.argmax(logits.to('cpu'), axis=1)\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predicted_labels += list(outputs)\n",
    "        correct_labels += list(label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "        \n",
    "    return eval_loss, correct_labels, predicted_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f5fb6-cce0-4d0c-ad6c-462889d98c11",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19e542-93c3-4865-89c6-8e5de2c7561e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 单轮 train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe7dcf-15dc-4607-aa0f-13b4bc950b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(year=2023, model_name=\"roberta\"):\n",
    "    print(\"model name\", model_name)\n",
    "    train_texts = []\n",
    "    dev_texts = []\n",
    "    train_labels = []\n",
    "    dev_labels = []\n",
    "    data_year_dir = join(settings.DATA_TRACE_DIR, \"PST\")\n",
    "    print(\"data_year_dir\", data_year_dir)\n",
    "\n",
    "    with open(join(data_year_dir, \"bib_context_train.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if is_clean: \n",
    "                train_texts.append(clean_text_line(line.strip()))\n",
    "            else: \n",
    "                train_texts.append(line.strip())\n",
    "    with open(join(data_year_dir, \"bib_context_valid.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if is_clean: \n",
    "                dev_texts.append(clean_text_line(line.strip()))\n",
    "            else: \n",
    "                dev_texts.append(line.strip())\n",
    "\n",
    "    with open(join(data_year_dir, \"bib_context_train_label.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            train_labels.append(int(line.strip()))\n",
    "    with open(join(data_year_dir, \"bib_context_valid_label.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            dev_labels.append(int(line.strip()))\n",
    "\n",
    "\n",
    "    print(\"Train size:\", len(train_texts))\n",
    "    print(\"Dev size:\", len(dev_texts))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    class_weight = len(train_labels) / (2 * np.bincount(train_labels))\n",
    "    class_weight = torch.Tensor(class_weight).to(device)\n",
    "    print(\"Class weight:\", class_weight)\n",
    "\n",
    "    if model_name == \"bert\":\n",
    "        BERT_MODEL = \"bert-base-uncased\"\n",
    "    elif model_name == \"scibert\":\n",
    "        # BERT_MODEL = \"allenai/scibert_scivocab_uncased\"\n",
    "        BERT_MODEL = './bert_models/scibert_scivocab_uncased/'\n",
    "    elif model_name == 'roberta-base': \n",
    "        BERT_MODEL = './bert_models/dsp_roberta_base_dapt_cs_tapt_sciie_3219/'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = 2)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "    train_features = convert_examples_to_inputs(train_texts, train_labels, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
    "    dev_features = convert_examples_to_inputs(dev_texts, dev_labels, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
    "    dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    GRADIENT_ACCUMULATION_STEPS = 2\n",
    "    NUM_TRAIN_EPOCHS = 20\n",
    "    LEARNING_RATE = 5e-5\n",
    "    WARMUP_PROPORTION = 0.1\n",
    "    MAX_GRAD_NORM = 5\n",
    "\n",
    "    num_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
    "    num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "    # optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\n",
    "    optimizer = get_optimizer(model)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    OUTPUT_DIR = join(settings.OUT_DIR, \"kddcup\", model_name)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "    PATIENCE = 5\n",
    "\n",
    "    loss_history = []\n",
    "    no_improvement = 0\n",
    "    \n",
    "    \n",
    "    for e in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        \n",
    "        ### learning_rate\n",
    "        lr = adjust_lr(optimizer, e)\n",
    "        \n",
    "        ### \n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)\n",
    "                # loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "                loss = criterion(logits, label_ids)\n",
    "\n",
    "            if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "            # loss.backward()\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM) \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                \n",
    "        dev_loss, _, _ = evaluate(model, dev_dataloader, device, criterion)\n",
    "        \n",
    "        print(\"Loss history:\", loss_history)\n",
    "        print(\"Dev loss:\", dev_loss)\n",
    "        \n",
    "        if len(loss_history) == 0 or dev_loss < min(loss_history):\n",
    "            no_improvement = 0\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "        \n",
    "        if no_improvement >= PATIENCE: \n",
    "            print(\"No improvement on development set. Finish training.\")\n",
    "            break\n",
    "            \n",
    "        loss_history.append(dev_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3021f9-824b-41ba-ae2c-2f7c6f88a924",
   "metadata": {
    "tags": []
   },
   "source": [
    "## kfold 训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca98d1-842f-412f-99e7-e1f055017a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_kfold(NFOLDS, year=2023, model_name=\"roberta\"):\n",
    "    print(\"model name\", model_name)\n",
    "    train_texts = []\n",
    "    dev_texts = []\n",
    "    train_labels = []\n",
    "    dev_labels = []\n",
    "    data_year_dir = join(settings.DATA_TRACE_DIR, \"PST\")\n",
    "    print(\"data_year_dir\", data_year_dir)\n",
    "\n",
    "    with open(join(data_year_dir, \"bib_context_train.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if is_clean: \n",
    "                train_texts.append(clean_text_line(line.strip()))\n",
    "            else: \n",
    "                train_texts.append(line.strip())\n",
    "    with open(join(data_year_dir, \"bib_context_valid.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if is_clean: \n",
    "                dev_texts.append(clean_text_line(line.strip()))\n",
    "            else: \n",
    "                dev_texts.append(line.strip())\n",
    "\n",
    "    with open(join(data_year_dir, \"bib_context_train_label.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            train_labels.append(int(line.strip()))\n",
    "    with open(join(data_year_dir, \"bib_context_valid_label.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            dev_labels.append(int(line.strip()))\n",
    "\n",
    "    print(\"Train size:\", len(train_texts))\n",
    "    print(\"Dev size:\", len(dev_texts))\n",
    "\n",
    "    ## pandas kfold \n",
    "    df_train_dev = pd.DataFrame()\n",
    "    df_train_dev['text'] = train_texts + dev_texts\n",
    "    df_train_dev['label'] = train_labels + dev_labels\n",
    "    print('df_train_dev shape', df_train_dev.shape)\n",
    "    ### 加载 模型 \n",
    "    if model_name == \"bert\":\n",
    "        BERT_MODEL = \"bert-base-uncased\"\n",
    "    elif model_name == \"scibert\":\n",
    "        BERT_MODEL = './bert_models/scibert_scivocab_uncased/'\n",
    "    elif model_name == 'roberta-base':\n",
    "        BERT_MODEL = './bert_models/dsp_roberta_base_dapt_cs_tapt_sciie_3219/'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device\", device)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = 2)\n",
    "    model.to(device)\n",
    "\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    NUM_TRAIN_EPOCHS = 20\n",
    "    LEARNING_RATE = 5e-5\n",
    "    WARMUP_PROPORTION = 0.1\n",
    "    MAX_GRAD_NORM = 5\n",
    "    BATCH_SIZE = 128\n",
    "\n",
    "    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df_train_dev)): \n",
    "        print('Fold ...', fold)\n",
    "        train_texts = df_train_dev.loc[train_idx, 'text'].tolist() \n",
    "        train_labels = df_train_dev.loc[train_idx, 'label'].tolist()\n",
    "\n",
    "        dev_texts = df_train_dev.loc[val_idx, 'text'].tolist() \n",
    "        dev_labels = df_train_dev.loc[val_idx, 'label'].tolist() \n",
    "\n",
    "        ## 设置标签的权重内容\n",
    "        class_weight = len(train_labels) / (2 * np.bincount(train_labels))\n",
    "        class_weight = torch.Tensor(class_weight).to(device)\n",
    "        print(\"Class weight:\", class_weight)\n",
    "        \n",
    "        ## 设置 loss 函数 \n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "        ## bert dataset \n",
    "        train_features = convert_examples_to_inputs(train_texts, train_labels, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
    "        dev_features = convert_examples_to_inputs(dev_texts, dev_labels, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "        ## bert dataloader \n",
    "        train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
    "        dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        ## 设置优化器的参数 \n",
    "        num_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
    "        num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
    "\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "\n",
    "        ## optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\n",
    "        optimizer = get_optimizer(model)\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        ### 文件写入 \n",
    "        OUTPUT_DIR = join(settings.OUT_DIR, \"kddcup\", model_name, f'num_fold={NFOLDS}',f'fold_{fold}')\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "        MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "        PATIENCE = 5\n",
    "\n",
    "        loss_history = []\n",
    "        no_improvement = 0\n",
    "        \n",
    "        ### 训练 epoch \n",
    "        for e in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "            model.train()\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "            ### learning_rate\n",
    "            lr = adjust_lr(optimizer, e)\n",
    "\n",
    "            ### \n",
    "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)\n",
    "                    logits = outputs[1]\n",
    "                    loss = criterion(logits, label_ids)\n",
    "\n",
    "                if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                    loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "\n",
    "                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM) \n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    scheduler.step()\n",
    "\n",
    "            dev_loss, _, _ = evaluate(model, dev_dataloader, device, criterion)\n",
    "\n",
    "            print(\"Loss history:\", loss_history)\n",
    "            print(\"Dev loss:\", dev_loss)\n",
    "\n",
    "            if len(loss_history) == 0 or dev_loss < min(loss_history):\n",
    "                no_improvement = 0\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if no_improvement >= PATIENCE: \n",
    "                print(\"No improvement on development set. Finish training.\")\n",
    "                break\n",
    "\n",
    "            loss_history.append(dev_loss)\n",
    "            \n",
    "            del loss, logits, outputs, batch, input_ids, input_mask, segment_ids, label_ids\n",
    "            \n",
    "        ### \n",
    "        print(f'fold_{fold} loss_history mean ... ', np.mean(loss_history)) \n",
    "        \n",
    "        ## \n",
    "        del train_dataloader, dev_dataloader\n",
    "        clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221429fd-bfc4-40ef-91a2-d8c7a087dec4",
   "metadata": {},
   "source": [
    "# 执行 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75b855-8b5f-4360-8300-4bfdbfb7335d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepare_bert_input()\n",
    "## 如果注释掉，则 fuzz-ratio-7则失效 \n",
    "\n",
    "# train(model_name=\"roberta\")\n",
    "train_kfold(NFOLDS, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b4809-1f8a-4bff-a2b9-22a0967c0a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5372d27-6c71-4c23-8ea8-1ec88e58e100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
