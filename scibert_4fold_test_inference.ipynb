{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effa17c6-5b87-41eb-aeee-7cdb735bfd5a",
   "metadata": {},
   "source": [
    "# Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892a5d5c-c579-4ccb-aa81-ffe2eebc682a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 15:34:14,532 Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-06-12 15:34:14,532 NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict as dd\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, DebertaV2ForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers.optimization import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, average_precision_score\n",
    "import logging\n",
    "import utils\n",
    "import settings\n",
    "### add \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold \n",
    "import gc\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "global is_clean, fuzz_ratio, model_name\n",
    "model_name = 'scibert'\n",
    "is_clean = False\n",
    "fuzz_ratio = 80\n",
    "NFOLDS = 4\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6240e-c5cf-46c5-9361-e1e686d7268a",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dde745a-2446-4ccf-b380-ea2ba4a1fa1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to clean RAM & vRAM\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    # ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def clean_text_line(line): \n",
    "    ## 得到 Abstract 信息\n",
    "    out_text = re.sub('<[^>]*>', '', line)\n",
    "    out_text = re.sub(r'\\t+', '\\t', out_text)\n",
    "    out_text = re.sub(r'\\n+', '\\n', out_text)\n",
    "    out_text = re.sub(r'[\\n\\t]+', '\\n', out_text)\n",
    "    return out_text\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, max_seq_length, tokenizer, verbose=0):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "        \n",
    "    return input_items\n",
    "\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size, num_workers=8)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588ed6f-cc10-421b-86a1-0c7408562b09",
   "metadata": {},
   "source": [
    "# 生成 test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e1114b-9b71-4dca-8161-ca8e1580627a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_kddcup_test_submission_bert(fold, model_name=\"deberta\"):\n",
    "    print(\"model name\", model_name)\n",
    "    data_dir = join(settings.DATA_TRACE_DIR, \"PST\")\n",
    "    papers = utils.load_json(data_dir, \"paper_source_trace_test_wo_ans.json\")\n",
    "\n",
    "    if model_name == \"bert\":\n",
    "        BERT_MODEL = \"bert-base-uncased\"\n",
    "    elif model_name == \"scibert\":\n",
    "        # BERT_MODEL = \"allenai/scibert_scivocab_uncased\"\n",
    "        BERT_MODEL = './bert_models/scibert_scivocab_uncased/'\n",
    "    elif model_name == 'roberta-base': \n",
    "        BERT_MODEL = './bert_models/dsp_roberta_base_dapt_cs_tapt_sciie_3219/'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "    sub_example_dict = utils.load_json(data_dir, \"submission_example_test.json\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device\", device)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = 2)\n",
    "    OUTPUT_DIR = join(settings.OUT_DIR, \"kddcup\", model_name, f'num_fold={NFOLDS}',f'fold_{fold}')\n",
    "    model.load_state_dict(torch.load(join(OUTPUT_DIR, \"pytorch_model.bin\")))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    xml_dir = join(data_dir, \"paper-xml\")\n",
    "    sub_dict = {}\n",
    "\n",
    "    for paper in tqdm(papers):\n",
    "        cur_pid = paper[\"_id\"]\n",
    "        file = join(xml_dir, cur_pid + \".xml\")\n",
    "        f = open(file, encoding='utf-8')\n",
    "        xml = f.read()\n",
    "        bs = BeautifulSoup(xml, \"xml\")\n",
    "        f.close()\n",
    "\n",
    "        references = bs.find_all(\"biblStruct\")\n",
    "        bid_to_title = {}\n",
    "        n_refs = 0\n",
    "        for ref in references:\n",
    "            if \"xml:id\" not in ref.attrs:\n",
    "                continue\n",
    "            bid = ref.attrs[\"xml:id\"]\n",
    "            if ref.analytic is None:\n",
    "                continue\n",
    "            if ref.analytic.title is None:\n",
    "                continue\n",
    "            bid_to_title[bid] = ref.analytic.title.text.lower()\n",
    "            b_idx = int(bid[1:]) + 1\n",
    "            if b_idx > n_refs:\n",
    "                n_refs = b_idx\n",
    "\n",
    "        bib_to_contexts = utils.find_bib_context(xml)\n",
    "        bib_sorted = [\"b\" + str(ii) for ii in range(n_refs)]\n",
    "        \n",
    "        y_score = [0] * n_refs\n",
    "\n",
    "        assert len(sub_example_dict[cur_pid]) == n_refs\n",
    "        # continue\n",
    "\n",
    "        contexts_sorted = [\" \".join(bib_to_contexts[bib]) for bib in bib_sorted]\n",
    "        contexts_sorted = [clean_text_line(line) for line in contexts_sorted]  ## 清洗数据 \n",
    "\n",
    "        test_features = convert_examples_to_inputs(contexts_sorted, y_score, MAX_SEQ_LENGTH, tokenizer)\n",
    "        test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        predicted_scores = []\n",
    "        for step, batch in enumerate(test_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                r = model(input_ids, attention_mask=input_mask,\n",
    "                                            token_type_ids=segment_ids, labels=label_ids)\n",
    "                tmp_eval_loss = r[0]\n",
    "                logits = r[1]\n",
    "\n",
    "            cur_pred_scores = logits[:, 1].to('cpu').numpy()\n",
    "            predicted_scores.extend(cur_pred_scores)\n",
    "        \n",
    "        for ii in range(len(predicted_scores)):\n",
    "            bib_idx = int(bib_sorted[ii][1:])\n",
    "            y_score[bib_idx] = float(utils.sigmoid(predicted_scores[ii]))\n",
    "        \n",
    "        sub_dict[cur_pid] = y_score\n",
    "    utils.dump_json(sub_dict, OUTPUT_DIR, f\"test_submission_{model_name}.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df52c76-5953-4849-852e-e77b5da3049c",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d438d46c-1630-4488-9431-d4adc7c1ee9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 15:36:21,480 loading paper_source_trace_test_wo_ans.json ...\n",
      "2024-06-12 15:36:21,489 paper_source_trace_test_wo_ans.json loaded\n",
      "2024-06-12 15:36:21,524 loading submission_example_test.json ...\n",
      "2024-06-12 15:36:21,526 submission_example_test.json loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name scibert\n",
      "device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_models/scibert_scivocab_uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                          | 0/394 [00:03<?, ?it/s]\n",
      "2024-06-12 15:36:26,357 dumping test_submission_scibert.json ...\n",
      "2024-06-12 15:36:26,358 test_submission_scibert.json dumped.\n",
      "2024-06-12 15:36:26,362 loading paper_source_trace_test_wo_ans.json ...\n",
      "2024-06-12 15:36:26,370 paper_source_trace_test_wo_ans.json loaded\n",
      "2024-06-12 15:36:26,398 loading submission_example_test.json ...\n",
      "2024-06-12 15:36:26,401 submission_example_test.json loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name scibert\n",
      "device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_models/scibert_scivocab_uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                          | 0/394 [00:03<?, ?it/s]\n",
      "2024-06-12 15:36:30,784 dumping test_submission_scibert.json ...\n",
      "2024-06-12 15:36:30,785 test_submission_scibert.json dumped.\n",
      "2024-06-12 15:36:30,790 loading paper_source_trace_test_wo_ans.json ...\n",
      "2024-06-12 15:36:30,799 paper_source_trace_test_wo_ans.json loaded\n",
      "2024-06-12 15:36:30,828 loading submission_example_test.json ...\n",
      "2024-06-12 15:36:30,830 submission_example_test.json loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name scibert\n",
      "device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_models/scibert_scivocab_uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                          | 0/394 [00:03<?, ?it/s]\n",
      "2024-06-12 15:36:35,411 dumping test_submission_scibert.json ...\n",
      "2024-06-12 15:36:35,412 test_submission_scibert.json dumped.\n",
      "2024-06-12 15:36:35,416 loading paper_source_trace_test_wo_ans.json ...\n",
      "2024-06-12 15:36:35,424 paper_source_trace_test_wo_ans.json loaded\n",
      "2024-06-12 15:36:35,453 loading submission_example_test.json ...\n",
      "2024-06-12 15:36:35,455 submission_example_test.json loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name scibert\n",
      "device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert_models/scibert_scivocab_uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                                                          | 0/394 [00:03<?, ?it/s]\n",
      "2024-06-12 15:36:40,055 dumping test_submission_scibert.json ...\n",
      "2024-06-12 15:36:40,057 test_submission_scibert.json dumped.\n",
      "2024-06-12 15:36:40,061 loading test_submission_scibert.json ...\n",
      "2024-06-12 15:36:40,061 test_submission_scibert.json loaded\n",
      "2024-06-12 15:36:40,062 loading test_submission_scibert.json ...\n",
      "2024-06-12 15:36:40,063 test_submission_scibert.json loaded\n",
      "2024-06-12 15:36:40,064 loading test_submission_scibert.json ...\n",
      "2024-06-12 15:36:40,065 test_submission_scibert.json loaded\n",
      "2024-06-12 15:36:40,066 loading test_submission_scibert.json ...\n",
      "2024-06-12 15:36:40,067 test_submission_scibert.json loaded\n",
      "2024-06-12 15:36:40,068 dumping test_submission_scibert.json ...\n",
      "2024-06-12 15:36:40,069 test_submission_scibert.json dumped.\n"
     ]
    }
   ],
   "source": [
    "for fold in range(NFOLDS): \n",
    "    gen_kddcup_test_submission_bert(fold, model_name=model_name)\n",
    "\n",
    "# 均值处理 \n",
    "\n",
    "import json\n",
    "fold = 0\n",
    "\n",
    "## 初始化 json 文件\n",
    "submit_data_dir = f'out/kddcup/{model_name}/num_fold={NFOLDS}/fold_{fold}'\n",
    "sub_example_dict = utils.load_json(submit_data_dir, f\"test_submission_{model_name}.json\")\n",
    "\n",
    "## 读取 剩余文件并进行 相加  \n",
    "for fold in range(1, NFOLDS): \n",
    "    ## 每次读取 \n",
    "    submit_data_dir = f'out/kddcup/{model_name}/num_fold={NFOLDS}/fold_{fold}'\n",
    "    sub_dict = utils.load_json(submit_data_dir, f\"test_submission_{model_name}.json\") \n",
    "    \n",
    "    ## 将 sub_dict[key] + sub_example_dict[key] \n",
    "    for key, value in sub_dict.items():\n",
    "        sub_example_dict[key] = np.add(sub_example_dict[key], value) \n",
    "\n",
    "## 进行均值处理  \n",
    "for key, value in sub_example_dict.items():\n",
    "    sub_example_dict[key] = (sub_example_dict[key] / NFOLDS).tolist()\n",
    "\n",
    "os.makedirs(f'out/kddcup/{model_name}/num_fold={NFOLDS}/fold_final', exist_ok=True)\n",
    "utils.dump_json(sub_example_dict, f'out/kddcup/{model_name}/num_fold={NFOLDS}/fold_final', f\"test_submission_{model_name}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e3255-a6da-4e94-88ac-efd84c739838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
